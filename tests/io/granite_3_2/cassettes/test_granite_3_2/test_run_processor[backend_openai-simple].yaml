interactions:
- request:
    body: '{"model":"granite3.2:2b","prompt":"<|start_of_role|>system<|end_of_role|>Knowledge
      Cutoff Date: April 2024.\nToday''s Date: July 17, 2025.\nYou are Granite, developed
      by IBM. You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Hello,
      how are you?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>I''m doing
      great. How can I help you today?<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>I''d
      like to show off how chat templating works!<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate, zstd
      connection:
      - keep-alive
      content-length:
      - '537'
      content-type:
      - application/json
      host:
      - localhost:11434
      user-agent:
      - AsyncOpenAI/Python 1.90.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - async:asyncio
      x-stainless-lang:
      - python
      x-stainless-os:
      - Linux
      x-stainless-package-version:
      - 1.90.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.13
    method: POST
    uri: http://localhost:11434/v1/completions
  response:
    body:
      string: '{"id":"cmpl-682","object":"text_completion","created":1752786039,"model":"granite3.2:2b","system_fingerprint":"fp_ollama","choices":[{"text":"Absolutely,
        I''d be happy to demonstrate how chat templates work with a simple example.
        Let''s consider we want to create a conversation template for greetings and
        asking basic questions about the day:\n\n1. **Template Initialization:**\n\n   ```python\n   day_greeting
        = (\n       \"Hello! How was your day today? I hope it started on a positive
        note.\\n\"\n       \"We''d love to hear how you''re feeling.\"\n   )\n   ```\n\n2.
        **User Input:**\n\n   Suppose the user inputs: \"Nice, thanks for asking!\"\n\n3.
        **Template Response:** With this conversation template, when the user says
        \"Nice, thanks for asking!\", our next step would be to respond with something
        contextually relevant from our template. \n\n   After parsing the input text,
        it might look like this (in Python pseudo code):\n   ```python\n   if response_prompt
        in user_input:\n       # Extract possible intent or key phrases \n       user_feelings
        = extract_intent(\"positive\")  # hypothetical function that analyzes feeling
        codes\n\n       response = f\"That''s great to hear, {user_feelings}! I hope
        it continues to stay positive.\"\n   else:\n       response = \"That''s wonderful!
        How was your day then?\"\n\n   print(f\"{day_greeting} {response}\")  # Outputs:
        \"Hello! How was your day? We''d love to hear how you''re feeling. That''s
        great to hear, positive! I hope it continues to stay positive.\"\n   ```\n\nWith
        this example, even though the user said something else than the exact expected
        input for a ''how were your feelings'', our template adapts by interpreting
        their response and crafting a personalized follow-up question based on it.
        This showcases how chat templates can make conversations more engaging and
        contextually appropriate.","index":0,"finish_reason":"stop"}],"usage":{"prompt_tokens":142,"completion_tokens":388,"total_tokens":530}}

        '
    headers:
      Content-Length:
      - '1964'
      Content-Type:
      - application/json
      Date:
      - Thu, 17 Jul 2025 21:00:39 GMT
    status:
      code: 200
      message: OK
version: 1
